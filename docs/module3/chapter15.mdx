---
title: "Chapter 15: Synthetic Data Generation"
description: "Generate massive annotated datasets for AI training using Isaac Sim Replicator, domain randomization, and cloud scaling"
sidebar_position: 15
---

# Chapter 15: Synthetic Data Generation

## Learning Objectives

By the end of this chapter, you will be able to:

- **Explain** the value of synthetic data for deep learning in robotics
- **Use** Isaac Sim Replicator API to programmatically generate scenes
- **Implement** Domain Randomization (DR) for robust sim-to-real transfer
- **Export** datasets in standard formats (COCO, KITTI, YOLO)
- **Parallelize** generation using headless mode and cloud scaling
- **Validate** synthetic data effectively for model training

**Prerequisites**: Chapter 12 (Isaac Sim Basics), Python knowledge  
**Estimated Time**: 75 minutes

---

## Why Synthetic Data?

**The Data Problem**: Training modern computer vision models (like YOLOv8 or Mask R-CNN) requires **thousands** of labeled images. 

**Manual Labeling**: Expensive, slow, inconsistent.  
**Synthetic Data**: Infinite, instant, perfectly labeled.

| Feature | Real Data | Synthetic Data |
|---------|-----------|----------------|
| **Cost** | High ($$ per image) | Low (Compute only) |
| **Speed** | Slow (Collection + Labeling) | Fast (Render speed) |
| **Labels** | Human error prone | Pixel-perfect ground truth |
| **Diversity** | Limited by physical access | Infinite (Weather, Lighting) |
| **Corner Cases** | Hard to capture (crashes) | Easy to simulate |

---

## Isaac Replicator Architecture

**Replicator** is built into Isaac Sim to automate data generation.

```mermaid
graph LR
    A[Replicator Script] --> B[Randomizers]
    A --> C[Writers]
    
    B --> D[Scene Graph<br/>(USD)]
    
    D --> E[Renderer<br/>(RTX)]
    
    E --> F[Annotators]
    F --> C
    
    C --> G[Dataset<br/>Images + JSON]
    
    style B fill:#F4A261
    style E fill:#76B900
    style F fill:#E76F51
```

**Key Concepts**:
1. **Randomizers**: Vary scene parameters (lighting, pose, texture, color).
2. **Annotators**: Extract Ground Truth (GT) like 2D bounding boxes, segmentation masks, depth maps.
3. **Writers**: Save data to disk in specific formats (e.g., KittiWriter).

---

## Building a Generation Pipeline

### Step 1: Basic Scene Setup

Start with a static scene (e.g., a warehouse).

```python
import omni.replicator.core as rep

# Create a camera
camera = rep.create.camera(position=(10, 10, 10), look_at=(0, 0, 0))

# Create a render product (image resolution)
render_product = rep.create.render_product(camera, (1024, 1024))
```

### Step 2: Defining Randomizers

We want to randomize the **Forklift** position and the **Lighting**.

```python
with rep.trigger.on_frame(num_frames=1000):
    # Randomize Sun Position
    with rep.create.light(rotation=(0, -45, 0), light_type="distant"):
        rep.modify.attribute("intensity", rep.distribution.uniform(500, 2000))
        rep.modify.pose(rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360)))

    # Randomize Objects
    # Get all pallets (assumes USD has "Pallet" in prim path)
    pallets = rep.get.prims(path_pattern=".*Pallet.*")
    with pallets:
        rep.modify.pose(
            position=rep.distribution.uniform((-5, -5, 0), (5, 5, 0)),
            rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360))
        )
        # Randomize Color
        rep.randomizer.color(colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1)))
```

### Step 3: Configuring Writers (The Annotations)

We want to train an object detector, so we need **Bounding Boxes** and **RGB** images.

```python
# Initialize BasicWriter
writer = rep.WriterRegistry.get("BasicWriter")

writer.initialize(
    output_dir="_output_dataset",
    rgb=True,
    bounding_box_2d_tight=True, # Improved fitting
    semantic_segmentation=True
)

# Attach render product to writer
writer.attach([render_product])
```

### Step 4: Running Generation

```python
# Run immediately
rep.orchestrator.run()
```

---

## Advanced Domain Randomization

To bridge the **Sim-to-Real gap**, we need aggressive randomization.

### Texture Randomization (Domain Randomization)

Randomizing textures prevents the model from overfitting to specific simulation assets.

```python
def randomize_textures():
    # Get all walls and floor
    structures = rep.get.prims(path_pattern="/World/Structure/.*")
    
    # List of 100+ materials
    materials = rep.get.material(path_pattern="/World/Materials/.*")
    
    with structures:
        rep.randomizer.materials(materials)

rep.randomizer.register(randomize_textures)
```

### Distractor Objects (Flying Distractors)

Add random flying geometric shapes to force the model to focus on the target objects, not just "anything in empty space."

```python
def randomize_distractors():
    shapes = rep.create.cube(count=10, scale=0.1)
    with shapes:
        rep.modify.pose(
            position=rep.distribution.uniform((-5,-5,0), (5,5,5)),
            rotation=rep.distribution.uniform((0,0,0), (360,360,360))
        )
        rep.modify.visibility(rep.distribution.choice([True, False]))
```

---

## Exporting for Training (YOLO/COCO)

Isaac Replicator supports custom writers.

### KITTI Writer Example

```python
kitti_writer = rep.WriterRegistry.get("KittiWriter")
kitti_writer.initialize(output_dir="kitti_dataset")
kitti_writer.attach([render_product])
```

### Custom YOLO Writer (Concept)

YOLO format requires: `class_id x_center y_center width height` (normalized).

You can write a custom Python script that reads the `bounding_box_2d_tight` output (npy/json) and converts it to `.txt` files expected by YOLOv8.

---

## Headless Generation & Cloud Scaling

Generating 100,000 images takes time. We run this **Headless** (no UI) to maximize speed.

### Running Headless

```bash
# Ubuntu terminal
./isaac-sim.sh --no-window --exec "path/to/my_generation_script.py"
```

**Speed Gain**: ~2-5x faster than GUI mode.

### Cloud Orchestration (AWS/Azure)

Containerize Isaac Sim using Docker:

```bash
docker pull nvcr.io/nvidia/isaac-sim:2023.1.1
docker run --gpus all -v $(pwd):/data nvcr.io/nvidia/isaac-sim:2023.1.1 \
  ./runheadless.native.sh --exec "generate.py"
```

Spin up 10 cloud instances ‚Üí Generate 1 Million images in an hour!

---

## Data Validation

Always inspect your synthetic data before training!

1. **Visual Check**: Open random images. Do they look realistic? Are labels accurate?
2. **Distribution Check**: Plot histogram of object classes. Is it balanced?
3. **Sim-to-Real Check**: Train a small model on synthetic data, test on a *real* validation set.

**Tools**: 
- **FiftyOne**: Open-source dataset visualization tool.
- **TensorBoard**: Visualize image summaries during generation.

---

## Hands-On Exercise

### Task: Generate "Hazard Detection" Dataset

**Scenario**: Generate a dataset of "Wet Floor Signs" and "Traffic Cones" in a warehouse for a safety robot.

1. **Assets**: Import Cones and Signs USDs (from NVIDIA Assets).
2. **Scene**: Warehouse environment.
3. **Randomization**:
   - Randomize lighting (morning, noon, evening).
   - Randomize number of cones (1-10) and positions.
   - Randomize camera angle (robot viewpoint).
4. **Output**: 500 RGB Images + 2D Bounding Boxes.

**Success Criteria**:
- Dataset folder created.
- JSON/NPY files contain valid bounding box coordinates.
- Objects are visible in the camera frame.

---

## Summary

### üéØ Key Takeaways

- **Synthetic Data** solves the data bottleneck for AI training.
- **Isaac Replicator** creates a fully programmable generation pipeline.
- **Domain Randomization** (DR) is essential for Sim-to-Real transfer.
- **Headless Mode** enables massive scaling on cloud infrastructure.
- **Writers** allow seamless export to standard formats like KITTI and COCO.

**Module 3 Completion**: You have now mastered the AI-Robot Brain! From simulation (Ch 11-12) to perception (Ch 13) to navigation (Ch 14) and finally, data generation (Ch 15).

**Module 4 Preview**: We will take this further by integrating **Vision-Language-Action (VLA)** models and **Generative AI** for truly intelligent humanoid behavior!

---

:::note Module Completion
‚úÖ You've completed Chapter 15: Synthetic Data Generation  
‚è±Ô∏è Estimated time to complete: 75 minutes  
üìä Progress: Module 3 - Chapter 5 of 5
:::
