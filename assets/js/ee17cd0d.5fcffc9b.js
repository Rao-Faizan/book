"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[556],{5295:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4/chapter18","title":"Chapter 18: Multi-Modal Robot Interaction","description":"Combine Hearing (Whisper), Seeing (VLMs), and Reasoning (LLMs) to create robots that understand gestures and spatial commands","source":"@site/docs/module4/chapter18.mdx","sourceDirName":"module4","slug":"/module4/chapter18","permalink":"/book/docs/module4/chapter18","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/book/tree/master/docs/module4/chapter18.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Chapter 18: Multi-Modal Robot Interaction","description":"Combine Hearing (Whisper), Seeing (VLMs), and Reasoning (LLMs) to create robots that understand gestures and spatial commands","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 17: LLMs for Cognitive Planning","permalink":"/book/docs/module4/chapter17"},"next":{"title":"Chapter 19: Capstone Project - Autonomous Humanoid","permalink":"/book/docs/module4/chapter19"}}');var t=s(4848),r=s(8453);const o={title:"Chapter 18: Multi-Modal Robot Interaction",description:"Combine Hearing (Whisper), Seeing (VLMs), and Reasoning (LLMs) to create robots that understand gestures and spatial commands",sidebar_position:3},l="Chapter 18: Multi-Modal Robot Interaction",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"The Power of &quot;Look at That&quot;",id:"the-power-of-look-at-that",level:2},{value:"Part 1: Vision-Language Models (VLMs)",id:"part-1-vision-language-models-vlms",level:2},{value:"Zero-Shot Detection with CLIP",id:"zero-shot-detection-with-clip",level:3},{value:"Part 2: Gesture Recognition",id:"part-2-gesture-recognition",level:2},{value:"Part 3: The Fusion Engine",id:"part-3-the-fusion-engine",level:2},{value:"Time Synchronization",id:"time-synchronization",level:3},{value:"The Fusion Logic",id:"the-fusion-logic",level:3},{value:"Part 4: Open-Vocabulary Object Detection (YOLO-World)",id:"part-4-open-vocabulary-object-detection-yolo-world",level:2},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Task: &quot;Fetch the [Color] Block&quot;",id:"task-fetch-the-color-block",level:3},{value:"Summary",id:"summary",level:2},{value:"\ud83c\udfaf Key Takeaways",id:"-key-takeaways",level:3}];function d(e){const n={admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-18-multi-modal-robot-interaction",children:"Chapter 18: Multi-Modal Robot Interaction"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand"})," the architecture of Vision-Language Models (VLMs) like CLIP and LLaVA"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," Gesture Recognition using Google MediaPipe"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fuse"}),' modalities: Combine "Look at ',(0,t.jsx)(n.em,{children:"that"}),'" (Voice) with a Pointing Gesture (Vision)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Identify"}),' objects using Open-Vocabulary detection (e.g., "Find the red toy")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build"})," a Multi-Modal ROS 2 Node that grounds language in the physical world"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create"})," a natural dialog system where the robot asks clarifying questions"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Chapter 16 (Voice), Chapter 17 (LLMs), Camera connected",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"Estimated Time"}),": 100 minutes"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"the-power-of-look-at-that",children:'The Power of "Look at That"'}),"\n",(0,t.jsxs)(n.p,{children:['In Chapter 17, our robot was "blind" to the semantic world. It could navigate to a ',(0,t.jsx)(n.em,{children:"kitchen"}),", but couldn't answer: ",(0,t.jsx)(n.em,{children:'"Is the stove on?"'})," or ",(0,t.jsx)(n.em,{children:'"Hand me the red cup."'})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Multi-Modal AI"})," bridges this gap by connecting:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Text"})," (Language)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Images"})," (Vision)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Proprioception"})," (Pointing/Gestures)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-1-vision-language-models-vlms",children:"Part 1: Vision-Language Models (VLMs)"}),"\n",(0,t.jsxs)(n.p,{children:["Traditional object detectors (YOLO) are limited to 80 COCO classes.\r\n",(0,t.jsx)(n.strong,{children:"VLMs"})," (like ",(0,t.jsx)(n.strong,{children:"CLIP"}),", ",(0,t.jsx)(n.strong,{children:"LLaVA"}),") can recognize ",(0,t.jsx)(n.em,{children:"anything"})," you can describe in text."]}),"\n",(0,t.jsx)(n.h3,{id:"zero-shot-detection-with-clip",children:"Zero-Shot Detection with CLIP"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CLIP"})," (Contrastive Language-Image Pretraining) tells you how well an image matches a text description."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Installation"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install ftfy regex tqdm git+https://github.com/openai/CLIP.git\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Python Usage"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import clip\r\nimport torch\r\nfrom PIL import Image\r\n\r\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\r\nmodel, preprocess = clip.load("ViT-B/32", device=device)\r\n\r\ndef classify_object(image_path, candidate_labels):\r\n    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\r\n    text = clip.tokenize(candidate_labels).to(device)\r\n\r\n    with torch.no_grad():\r\n        logits_per_image, _ = model(image, text)\r\n        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\r\n\r\n    # Returns list of probabilities matching labels\r\n    return probs[0]\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robot Use Case"}),':\r\nUser: "Find the ',(0,t.jsx)(n.strong,{children:"spicy sauce"}),'."\r\nRobot: Scans shelf, crops objects, runs CLIP against ["spicy sauce", "ketchup", "mustard"]. Highest score wins.']}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-2-gesture-recognition",children:"Part 2: Gesture Recognition"}),"\n",(0,t.jsxs)(n.p,{children:["Pointing is a powerful deictic gesture. ",(0,t.jsx)(n.em,{children:'"Go there"'})," means nothing without the gesture."]}),"\n",(0,t.jsxs)(n.p,{children:["We use ",(0,t.jsx)(n.strong,{children:"Google MediaPipe"})," for real-time hand tracking."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Installation"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install mediapipe\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pointing Direction Node"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import mediapipe as mp\r\nimport cv2\r\nimport numpy as np\r\n\r\n# Initialize MediaPipe Hands\r\nmp_hands = mp.solutions.hands\r\nhands = mp_hands.Hands(max_num_hands=1)\r\n\r\ndef get_pointing_vector(image):\r\n    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n    \r\n    if results.multi_hand_landmarks:\r\n        lm = results.multi_hand_landmarks[0]\r\n        \r\n        # Index Finger: Base (5) to Tip (8)\r\n        base = np.array([lm.landmark[5].x, lm.landmark[5].y])\r\n        tip = np.array([lm.landmark[8].x, lm.landmark[8].y])\r\n        \r\n        # 2D Vector\r\n        vector = tip - base\r\n        return vector # (dx, dy)\r\n        \r\n    return None\n"})}),"\n",(0,t.jsxs)(n.p,{children:["To map 2D pointing to 3D world coordinates, you need ",(0,t.jsx)(n.strong,{children:"Depth"})," (RealSense) and intrinsic calibration. Conceptually, you cast a ray from the camera center through the finger vector until it hits the floor."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-3-the-fusion-engine",children:"Part 3: The Fusion Engine"}),"\n",(0,t.jsxs)(n.p,{children:["We need a node that listens to ",(0,t.jsx)(n.strong,{children:"Voice"})," and watches ",(0,t.jsx)(n.strong,{children:"Vision"})," simultaneously."]}),"\n",(0,t.jsx)(n.h3,{id:"time-synchronization",children:"Time Synchronization"}),"\n",(0,t.jsxs)(n.p,{children:['If I say "That one" at ',(0,t.jsx)(n.code,{children:"t=10.5"}),"s, I was probably pointing at ",(0,t.jsx)(n.code,{children:"t=10.0-11.0"}),"s.\r\nExact timestamp matching in ROS 2 (",(0,t.jsx)(n.code,{children:"message_filters.ApproximateTimeSynchronizer"}),") is critical."]}),"\n",(0,t.jsx)(n.h3,{id:"the-fusion-logic",children:"The Fusion Logic"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),': "Pick up [DEICTIC_REF]" (e.g., "this", "that", "it")\r\n',(0,t.jsx)(n.strong,{children:"Trigger"}),": Gesture Detection."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MultiModalFusion(Node):\r\n    def __init__(self):\r\n        # ... setup subscribers ...\r\n        self.last_voice_cmd = None\r\n        self.last_gesture_vec = None\r\n        \r\n    def fuse(self):\r\n        if "that" in self.last_voice_cmd:\r\n            if self.last_gesture_vec:\r\n                target_loc = self.project_ray(self.last_gesture_vec)\r\n                return f"navigate({target_loc})"\r\n            else:\r\n                return "speak(\'I didn\'t see where you pointed.\')"\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-4-open-vocabulary-object-detection-yolo-world",children:"Part 4: Open-Vocabulary Object Detection (YOLO-World)"}),"\n",(0,t.jsxs)(n.p,{children:["Newer models like ",(0,t.jsx)(n.strong,{children:"YOLO-World"})," allow you to define classes at runtime."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prompt"}),': "Find the ',(0,t.jsx)(n.strong,{children:"safety vest"}),'."\r\n',(0,t.jsx)(n.strong,{children:"YOLO-World"}),": Detects person, but specifically highlights the vest."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Integration"}),":\r\nUse ",(0,t.jsx)(n.code,{children:"isaac_ros_yolov8"})," (Module 3) but swap the model weights for a fine-tuned open-vocabulary model or standard YOLO if classes match."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,t.jsx)(n.h3,{id:"task-fetch-the-color-block",children:'Task: "Fetch the [Color] Block"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scenario"}),": Table with Red, Green, and Blue blocks.\r\n",(0,t.jsx)(n.strong,{children:"Command"}),': "Pick up the ',(0,t.jsx)(n.strong,{children:"Green"}),' block." (Uses VLM/CLIP).\r\n',(0,t.jsx)(n.strong,{children:"Command"}),': "Pick up ',(0,t.jsx)(n.strong,{children:"that"}),' block." (Uses Gesture).']}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Setup"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"RGB Camera looking at table."}),"\n",(0,t.jsx)(n.li,{children:"Microphone."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice"}),": Whisper transcribes command."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM"}),": Extracts ",(0,t.jsx)(n.code,{children:'target_object="green block"'}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Object Detector finds 3 blocks."}),"\n",(0,t.jsx)(n.li,{children:"Crop images of 3 blocks."}),"\n",(0,t.jsx)(n.li,{children:'CLIP compares crops to "green block".'}),"\n",(0,t.jsx)(n.li,{children:"Highest confidence is selected."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Arm moves to pick it up."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot correctly disambiguates objects based on color/description."}),"\n",(0,t.jsx)(n.li,{children:'Robot correctly identifies "that" based on pointing (Left vs Right).'}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.h3,{id:"-key-takeaways",children:"\ud83c\udfaf Key Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Modal AI"})," allows interacting with robots like humans interact with each other."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VLMs (CLIP)"})," act as a universal translator between Text and Pixels."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MediaPipe"})," provides low-cost, high-speed gesture recognition."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deictic Resolution"}),' ("this", "there") requires fusing timestamped voice and vision data.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Closed-Loop Interaction"}),': The robot should ask "Did you mean the red one?" if unsure.']}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next Chapter"}),": We have all the pieces: ROS 2 (Body), Simulation (World), Isaac/Nav2 (Skills), Voice (Ears), LLM (Brain), and VLM (Eyes).\r\nIn ",(0,t.jsx)(n.strong,{children:"Chapter 19"}),", we combine EVERYTHING into the ",(0,t.jsx)(n.strong,{children:"Capstone Project: The Autonomous Humanoid"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.admonition,{title:"Chapter Completion",type:"note",children:(0,t.jsxs)(n.p,{children:["\u2705 You've completed Chapter 18: Multi-Modal Robot Interaction",(0,t.jsx)(n.br,{}),"\n","\u23f1\ufe0f Estimated time to complete: 100 minutes",(0,t.jsx)(n.br,{}),"\n","\ud83d\udcca Progress: Module 4 - Chapter 3 of 5"]})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var i=s(6540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);