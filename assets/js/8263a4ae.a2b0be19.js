"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[2761],{682:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module2/chapter9","title":"Chapter 9: Sensor Simulation","description":"Simulate robotic sensors including cameras, LiDAR, depth sensors, and IMUs with realistic noise models in Gazebo and Unity","source":"@site/docs/module2/chapter9.mdx","sourceDirName":"module2","slug":"/module2/chapter9","permalink":"/book/docs/module2/chapter9","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/book/tree/master/docs/module2/chapter9.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Chapter 9: Sensor Simulation","description":"Simulate robotic sensors including cameras, LiDAR, depth sensors, and IMUs with realistic noise models in Gazebo and Unity","sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Unity for High-Fidelity Rendering","permalink":"/book/docs/module2/chapter8"},"next":{"title":"Chapter 10: Sim-to-Real Transfer","permalink":"/book/docs/module2/chapter10"}}');var a=r(4848),s=r(8453);const t={title:"Chapter 9: Sensor Simulation",description:"Simulate robotic sensors including cameras, LiDAR, depth sensors, and IMUs with realistic noise models in Gazebo and Unity",sidebar_position:9},l="Chapter 9: Sensor Simulation",o={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Why Sensor Simulation Matters",id:"why-sensor-simulation-matters",level:2},{value:"Overview of Robotic Sensors",id:"overview-of-robotic-sensors",level:2},{value:"Common Sensor Types",id:"common-sensor-types",level:3},{value:"Sensor Data Flow",id:"sensor-data-flow",level:3},{value:"RGB Camera Simulation",id:"rgb-camera-simulation",level:2},{value:"Gazebo: RGB Camera Plugin",id:"gazebo-rgb-camera-plugin",level:3},{value:"Unity: RGB Camera Setup",id:"unity-rgb-camera-setup",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Gazebo: Depth Camera Plugin",id:"gazebo-depth-camera-plugin",level:3},{value:"Unity: Depth Camera with Perception Package",id:"unity-depth-camera-with-perception-package",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"2D LiDAR (Single Plane Scan)",id:"2d-lidar-single-plane-scan",level:3},{value:"3D LiDAR (Multi-Layer Scan)",id:"3d-lidar-multi-layer-scan",level:3},{value:"IMU (Inertial Measurement Unit) Simulation",id:"imu-inertial-measurement-unit-simulation",level:2},{value:"Gazebo IMU Plugin",id:"gazebo-imu-plugin",level:3},{value:"Sensor Noise Modeling",id:"sensor-noise-modeling",level:2},{value:"Types of Noise",id:"types-of-noise",level:3},{value:"Gaussian Noise Example",id:"gaussian-noise-example",level:3},{value:"Bias and Drift",id:"bias-and-drift",level:3},{value:"Custom Noise in Unity",id:"custom-noise-in-unity",level:3},{value:"Multi-Sensor Integration",id:"multi-sensor-integration",level:2},{value:"Example: Camera + LiDAR + IMU",id:"example-camera--lidar--imu",level:3},{value:"RViz2 Visualization Configuration",id:"rviz2-visualization-configuration",level:2},{value:"Complete RViz2 Setup for Sensor Debugging",id:"complete-rviz2-setup-for-sensor-debugging",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Sensor Update Rate vs. Real-Time Factor",id:"sensor-update-rate-vs-real-time-factor",level:3},{value:"Recommended Update Rates",id:"recommended-update-rates",level:3},{value:"Gazebo Performance Tips",id:"gazebo-performance-tips",level:3},{value:"Complete Sensor Suite Example",id:"complete-sensor-suite-example",level:2},{value:"Humanoid with Full Sensor Array",id:"humanoid-with-full-sensor-array",level:3},{value:"Summary",id:"summary",level:2},{value:"\ud83c\udfaf Key Takeaways",id:"-key-takeaways",level:3},{value:"\ud83d\udcca Sensor Comparison",id:"-sensor-comparison",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Task: Create Multi-Sensor Humanoid",id:"task-create-multi-sensor-humanoid",level:3},{value:"Further Resources",id:"further-resources",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Tutorials",id:"tutorials",level:3},{value:"Tools",id:"tools",level:3},{value:"What&#39;s Next?",id:"whats-next",level:2}];function c(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-9-sensor-simulation",children:"Chapter 9: Sensor Simulation"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Configure"})," RGB and depth cameras in Gazebo and Unity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulate"})," 2D and 3D LiDAR sensors with point cloud output"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement"})," IMU (Inertial Measurement Unit) sensors with realistic noise"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Model"})," sensor noise (Gaussian, bias, drift) for sim-to-real transfer"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visualize"})," sensor data in RViz2 for debugging"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integrate"})," multiple sensors for perception pipelines"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Optimize"})," sensor update rates for real-time performance"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prerequisites"}),": Chapters 6-8 (Gazebo, Unity fundamentals)",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Estimated Time"}),": 75 minutes"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"why-sensor-simulation-matters",children:"Why Sensor Simulation Matters"}),"\n",(0,a.jsx)(n.p,{children:"Robots perceive the world through sensors. Before deploying on real hardware, you need to:"}),"\n",(0,a.jsxs)(n.p,{children:["\u2705 ",(0,a.jsx)(n.strong,{children:"Test perception algorithms"})," without expensive sensors",(0,a.jsx)(n.br,{}),"\n","\u2705 ",(0,a.jsx)(n.strong,{children:"Generate training data"})," for machine learning models",(0,a.jsx)(n.br,{}),"\n","\u2705 ",(0,a.jsx)(n.strong,{children:"Validate sensor placement"})," (field of view, occlusions)",(0,a.jsx)(n.br,{}),"\n","\u2705 ",(0,a.jsx)(n.strong,{children:"Understand noise characteristics"})," for robust algorithm design",(0,a.jsx)(n.br,{}),"\n","\u2705 ",(0,a.jsx)(n.strong,{children:"Debug perception pipelines"})," in controlled environments"]}),"\n",(0,a.jsx)(n.admonition,{title:"Industry Practice",type:"tip",children:(0,a.jsxs)(n.p,{children:["Waymo generates ",(0,a.jsx)(n.strong,{children:"billions of synthetic sensor images"})," in simulation before testing on real cars. Simulation accelerates sensor testing by 1000x."]})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"overview-of-robotic-sensors",children:"Overview of Robotic Sensors"}),"\n",(0,a.jsx)(n.h3,{id:"common-sensor-types",children:"Common Sensor Types"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Sensor"}),(0,a.jsx)(n.th,{children:"Output"}),(0,a.jsx)(n.th,{children:"Use Case"}),(0,a.jsx)(n.th,{children:"Range"}),(0,a.jsx)(n.th,{children:"Cost (Real)"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"RGB Camera"})}),(0,a.jsx)(n.td,{children:"Color images"}),(0,a.jsx)(n.td,{children:"Object detection, tracking"}),(0,a.jsx)(n.td,{children:"1-50m"}),(0,a.jsx)(n.td,{children:"$50-500"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Depth Camera"})}),(0,a.jsx)(n.td,{children:"RGB + Depth map"}),(0,a.jsx)(n.td,{children:"3D reconstruction, SLAM"}),(0,a.jsx)(n.td,{children:"0.5-10m"}),(0,a.jsx)(n.td,{children:"$200-400"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"2D LiDAR"})}),(0,a.jsx)(n.td,{children:"2D point cloud"}),(0,a.jsx)(n.td,{children:"Navigation, mapping"}),(0,a.jsx)(n.td,{children:"0.1-30m"}),(0,a.jsx)(n.td,{children:"$100-1000"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"3D LiDAR"})}),(0,a.jsx)(n.td,{children:"3D point cloud"}),(0,a.jsx)(n.td,{children:"Autonomous driving, mapping"}),(0,a.jsx)(n.td,{children:"0.1-100m"}),(0,a.jsx)(n.td,{children:"$1000-75000"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"IMU"})}),(0,a.jsx)(n.td,{children:"Acceleration, angular velocity"}),(0,a.jsx)(n.td,{children:"Localization, balance"}),(0,a.jsx)(n.td,{children:"N/A"}),(0,a.jsx)(n.td,{children:"$10-500"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Force-Torque"})}),(0,a.jsx)(n.td,{children:"Forces, torques"}),(0,a.jsx)(n.td,{children:"Manipulation, contact detection"}),(0,a.jsx)(n.td,{children:"N/A"}),(0,a.jsx)(n.td,{children:"$1000-5000"})]})]})]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-data-flow",children:"Sensor Data Flow"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph LR\r\n    A[Physical World] --\x3e B[Sensor Model]\r\n    B --\x3e C[Add Noise]\r\n    C --\x3e D[ROS 2 Topic]\r\n    D --\x3e E[Perception Algorithm]\r\n    E --\x3e F[Robot Action]\r\n    \r\n    style B fill:#95C8D8\r\n    style C fill:#F4A261\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"rgb-camera-simulation",children:"RGB Camera Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"gazebo-rgb-camera-plugin",children:"Gazebo: RGB Camera Plugin"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Add to URDF:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Camera link --\x3e\r\n<link name="camera_link">\r\n  <visual>\r\n    <geometry>\r\n      <box size="0.05 0.05 0.05"/>\r\n    </geometry>\r\n    <material name="blue"/>\r\n  </visual>\r\n  <collision>\r\n    <geometry>\r\n      <box size="0.05 0.05 0.05"/>\r\n    </geometry>\r\n  </collision>\r\n  <inertial>\r\n    <mass value="0.1"/>\r\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\r\n  </inertial>\r\n</link>\r\n\r\n\x3c!-- Joint connecting camera to robot head --\x3e\r\n<joint name="camera_joint" type="fixed">\r\n  <parent link="head_link"/>\r\n  <child link="camera_link"/>\r\n  <origin xyz="0.1 0 0.05" rpy="0 0 0"/>\r\n</joint>\r\n\r\n\x3c!-- Gazebo camera plugin --\x3e\r\n<gazebo reference="camera_link">\r\n  <sensor name="camera" type="camera">\r\n    <update_rate>30.0</update_rate>\r\n    <camera name="head_camera">\r\n      <horizontal_fov>1.3962634</horizontal_fov>  \x3c!-- 80 degrees --\x3e\r\n      <image>\r\n        <width>640</width>\r\n        <height>480</height>\r\n        <format>R8G8B8</format>\r\n      </image>\r\n      <clip>\r\n        <near>0.02</near>\r\n        <far>300</far>\r\n      </clip>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.007</stddev>  \x3c!-- Realistic camera noise --\x3e\r\n      </noise>\r\n    </camera>\r\n    \r\n    \x3c!-- ROS 2 integration --\x3e\r\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n      <ros>\r\n        <namespace>camera</namespace>\r\n        <remapping>image_raw:=rgb/image_raw</remapping>\r\n        <remapping>camera_info:=rgb/camera_info</remapping>\r\n      </ros>\r\n      <camera_name>head_camera</camera_name>\r\n      <frame_name>camera_link</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Test in Gazebo:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Launch Gazebo with robot\r\nros2 launch my_robot_description spawn_robot.launch.py\r\n\r\n# View camera topics\r\nros2 topic list | grep camera\r\n# Output: /camera/rgb/image_raw, /camera/rgb/camera_info\r\n\r\n# Visualize camera feed\r\nros2 run rqt_image_view rqt_image_view /camera/rgb/image_raw\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"unity-rgb-camera-setup",children:"Unity: RGB Camera Setup"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Add Camera Component:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\r\nusing Unity.Robotics.ROSTCPConnector;\r\nusing RosMessageTypes.Sensor;\r\n\r\npublic class RGBCameraPublisher : MonoBehaviour\r\n{\r\n    ROSConnection ros;\r\n    public Camera rgbCamera;\r\n    public string topicName = "camera/rgb/image_raw";\r\n    public int publishRate = 30; // Hz\r\n    \r\n    private RenderTexture renderTexture;\r\n    private Texture2D texture2D;\r\n    \r\n    void Start()\r\n    {\r\n        ros = ROSConnection.GetOrCreateInstance();\r\n        ros.RegisterPublisher<ImageMsg>(topicName);\r\n        \r\n        // Setup render texture\r\n        renderTexture = new RenderTexture(640, 480, 24);\r\n        rgbCamera.targetTexture = renderTexture;\r\n        texture2D = new Texture2D(640, 480, TextureFormat.RGB24, false);\r\n        \r\n        InvokeRepeating("PublishImage", 0f, 1f / publishRate);\r\n    }\r\n    \r\n    void PublishImage()\r\n    {\r\n        // Read pixels from render texture\r\n        RenderTexture.active = renderTexture;\r\n        texture2D.ReadPixels(new Rect(0, 0, 640, 480), 0, 0);\r\n        texture2D.Apply();\r\n        \r\n        // Convert to ROS message\r\n        ImageMsg msg = new ImageMsg\r\n        {\r\n            header = new HeaderMsg \r\n            { \r\n                stamp = new TimeMsg(), \r\n                frame_id = "camera_link" \r\n            },\r\n            height = 480,\r\n            width = 640,\r\n            encoding = "rgb8",\r\n            is_bigendian = 0,\r\n            step = 640 * 3,\r\n            data = texture2D.GetRawTextureData()\r\n        };\r\n        \r\n        ros.Publish(topicName, msg);\r\n    }\r\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Add Realistic Camera Effects:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:"// Add to Camera component\r\nusing UnityEngine.Rendering.PostProcessing;\r\n\r\nPostProcessVolume volume = gameObject.AddComponent<PostProcessVolume>();\r\nvolume.isGlobal = true;\r\n\r\nvar profile = ScriptableObject.CreateInstance<PostProcessProfile>();\r\n\r\n// Add bloom (lens flare effect)\r\nvar bloom = profile.AddSettings<Bloom>();\r\nbloom.intensity.value = 0.5f;\r\n\r\n// Add grain (sensor noise)\r\nvar grain = profile.AddSettings<Grain>();\r\ngrain.intensity.value = 0.3f;\r\ngrain.size.value = 1.0f;\r\n\r\nvolume.profile = profile;\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras (like Intel RealSense D435i) output RGB + depth information."}),"\n",(0,a.jsx)(n.h3,{id:"gazebo-depth-camera-plugin",children:"Gazebo: Depth Camera Plugin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="depth_camera_link">\r\n  <sensor name="depth_camera" type="depth">\r\n    <update_rate>30.0</update_rate>\r\n    <camera name="depth_camera">\r\n      <horizontal_fov>1.5708</horizontal_fov>  \x3c!-- 90 degrees --\x3e\r\n      <image>\r\n        <width>640</width>\r\n        <height>480</height>\r\n        <format>R8G8B8</format>\r\n      </image>\r\n      <clip>\r\n        <near>0.3</near>  \x3c!-- Min depth: 30cm --\x3e\r\n        <far>10.0</far>   \x3c!-- Max depth: 10m --\x3e\r\n      </clip>\r\n    </camera>\r\n    \r\n    <plugin name="depth_controller" filename="libgazebo_ros_camera.so">\r\n      <ros>\r\n        <namespace>depth_camera</namespace>\r\n        <remapping>image_raw:=rgb/image_raw</remapping>\r\n        <remapping>depth/image_raw:=depth/image_raw</remapping>\r\n        <remapping>points:=points</remapping>\r\n      </ros>\r\n      <camera_name>depth_camera</camera_name>\r\n      <frame_name>depth_camera_link</frame_name>\r\n      <hack_baseline>0.07</hack_baseline>  \x3c!-- Stereo baseline --\x3e\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Accessing Depth Data:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass DepthProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('depth_processor')\r\n        self.bridge = CvBridge()\r\n        \r\n        self.sub = self.create_subscription(\r\n            Image,\r\n            '/depth_camera/depth/image_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n    \r\n    def depth_callback(self, msg):\r\n        # Convert ROS Image to numpy array\r\n        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\r\n        \r\n        # Depth is in meters (float32)\r\n        # Example: Get depth at center pixel\r\n        center_x, center_y = depth_image.shape[1] // 2, depth_image.shape[0] // 2\r\n        center_depth = depth_image[center_y, center_x]\r\n        \r\n        self.get_logger().info(f'Depth at center: {center_depth:.2f} meters')\r\n        \r\n        # Visualize depth (convert to 8-bit for display)\r\n        depth_colormap = cv2.applyColorMap(\r\n            cv2.convertScaleAbs(depth_image, alpha=25), \r\n            cv2.COLORMAP_JET\r\n        )\r\n        cv2.imshow('Depth Map', depth_colormap)\r\n        cv2.waitKey(1)\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"unity-depth-camera-with-perception-package",children:"Unity: Depth Camera with Perception Package"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine.Perception.GroundTruth;\r\n\r\npublic class DepthCameraSetup : MonoBehaviour\r\n{\r\n    void Start()\r\n    {\r\n        Camera cam = GetComponent<Camera>();\r\n        \r\n        // Add Perception Camera\r\n        PerceptionCamera perceptionCam = cam.gameObject.AddComponent<PerceptionCamera>();\r\n        \r\n        // Enable depth rendering\r\n        cam.depthTextureMode = DepthTextureMode.Depth;\r\n        \r\n        // Add Instance Segmentation (outputs depth + object IDs)\r\n        InstanceSegmentationLabeler segLabeler = new InstanceSegmentationLabeler();\r\n        perceptionCam.AddLabeler(segLabeler);\r\n        \r\n        Debug.Log("Depth camera ready!");\r\n    }\r\n}\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,a.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) emits laser beams and measures distances."}),"\n",(0,a.jsx)(n.h3,{id:"2d-lidar-single-plane-scan",children:"2D LiDAR (Single Plane Scan)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Gazebo Configuration:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="lidar_link">\r\n  <sensor name="lidar_2d" type="ray">\r\n    <update_rate>10.0</update_rate>\r\n    <ray>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>360</samples>  \x3c!-- 360 samples = 1\xb0 resolution --\x3e\r\n          <resolution>1</resolution>\r\n          <min_angle>-3.14159</min_angle>  \x3c!-- -180\xb0 --\x3e\r\n          <max_angle>3.14159</max_angle>   \x3c!-- +180\xb0 --\x3e\r\n        </horizontal>\r\n      </scan>\r\n      <range>\r\n        <min>0.1</min>  \x3c!-- 10cm min range --\x3e\r\n        <max>30.0</max> \x3c!-- 30m max range --\x3e\r\n        <resolution>0.01</resolution>\r\n      </range>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\r\n      </noise>\r\n    </ray>\r\n    \r\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\r\n      <ros>\r\n        <namespace>lidar</namespace>\r\n        <remapping>~/out:=scan</remapping>\r\n      </ros>\r\n      <output_type>sensor_msgs/LaserScan</output_type>\r\n      <frame_name>lidar_link</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Visualize in RViz2:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Launch RViz2\r\nrviz2\r\n\r\n# Add LaserScan display\r\n# Fixed Frame: base_link\r\n# Topic: /lidar/scan\r\n# Size: 0.05\r\n# Color: Red\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"3d-lidar-multi-layer-scan",children:"3D LiDAR (Multi-Layer Scan)"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Gazebo 3D LiDAR:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="lidar_3d_link">\r\n  <sensor name="lidar_3d" type="gpu_ray">  \x3c!-- GPU-accelerated --\x3e\r\n    <update_rate>10.0</update_rate>\r\n    <ray>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>1024</samples>  \x3c!-- Horizontal resolution --\x3e\r\n          <min_angle>-3.14159</min_angle>\r\n          <max_angle>3.14159</max_angle>\r\n        </horizontal>\r\n        <vertical>\r\n          <samples>64</samples>  \x3c!-- 64 vertical layers --\x3e\r\n          <min_angle>-0.5236</min_angle>  \x3c!-- -30\xb0 --\x3e\r\n          <max_angle>0.1745</max_angle>   \x3c!-- +10\xb0 --\x3e\r\n        </vertical>\r\n      </scan>\r\n      <range>\r\n        <min>0.5</min>\r\n        <max>100.0</max>\r\n      </range>\r\n    </ray>\r\n    \r\n    <plugin name="lidar_3d_controller" filename="libgazebo_ros_ray_sensor.so">\r\n      <ros>\r\n        <namespace>lidar_3d</namespace>\r\n        <remapping>~/out:=points</remapping>\r\n      </ros>\r\n      <output_type>sensor_msgs/PointCloud2</output_type>\r\n      <frame_name>lidar_3d_link</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Processing Point Cloud:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import PointCloud2\r\nimport sensor_msgs_py.point_cloud2 as pc2\r\nimport numpy as np\r\n\r\nclass PointCloudProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('pointcloud_processor')\r\n        self.sub = self.create_subscription(\r\n            PointCloud2,\r\n            '/lidar_3d/points',\r\n            self.pointcloud_callback,\r\n            10\r\n        )\r\n    \r\n    def pointcloud_callback(self, msg):\r\n        # Convert to numpy array\r\n        points = pc2.read_points(msg, field_names=(\"x\", \"y\", \"z\"), skip_nans=True)\r\n        points_array = np.array(list(points))\r\n        \r\n        # Example: Filter points within 5m\r\n        distances = np.linalg.norm(points_array, axis=1)\r\n        close_points = points_array[distances < 5.0]\r\n        \r\n        self.get_logger().info(f'Points within 5m: {len(close_points)}')\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"RViz2 PointCloud2 Visualization:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# In RViz2\r\nAdd \u2192 PointCloud2\r\nTopic: /lidar_3d/points\r\nStyle: Points\r\nSize: 0.02\r\nColor Transformer: AxisColor (Z-axis)\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"imu-inertial-measurement-unit-simulation",children:"IMU (Inertial Measurement Unit) Simulation"}),"\n",(0,a.jsx)(n.p,{children:"IMU provides acceleration and angular velocity data for localization."}),"\n",(0,a.jsx)(n.h3,{id:"gazebo-imu-plugin",children:"Gazebo IMU Plugin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="imu_link">\r\n  <sensor name="imu_sensor" type="imu">\r\n    <update_rate>100.0</update_rate>  \x3c!-- High rate for control --\x3e\r\n    <imu>\r\n      <angular_velocity>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0002</stddev>  \x3c!-- Gyroscope noise --\x3e\r\n            <bias_mean>0.0001</bias_mean>\r\n            <bias_stddev>0.00001</bias_stddev>\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0002</stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0002</stddev>\r\n          </noise>\r\n        </z>\r\n      </angular_velocity>\r\n      \r\n      <linear_acceleration>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.017</stddev>  \x3c!-- Accelerometer noise --\x3e\r\n            <bias_mean>0.1</bias_mean>\r\n            <bias_stddev>0.001</bias_stddev>\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.017</stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.017</stddev>\r\n          </noise>\r\n        </z>\r\n      </linear_acceleration>\r\n    </imu>\r\n    \r\n    <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\r\n      <ros>\r\n        <namespace>imu</namespace>\r\n        <remapping>~/out:=data</remapping>\r\n      </ros>\r\n      <frame_name>imu_link</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Reading IMU Data:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\r\n\r\nclass IMUReader(Node):\r\n    def __init__(self):\r\n        super().__init__('imu_reader')\r\n        self.sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\r\n    \r\n    def imu_callback(self, msg):\r\n        # Orientation (quaternion)\r\n        quat = msg.orientation\r\n        \r\n        # Angular velocity (rad/s)\r\n        angular_vel = msg.angular_velocity\r\n        \r\n        # Linear acceleration (m/s\xb2)\r\n        linear_acc = msg.linear_acceleration\r\n        \r\n        self.get_logger().info(\r\n            f'IMU - Accel: ({linear_acc.x:.2f}, {linear_acc.y:.2f}, {linear_acc.z:.2f})'\r\n        )\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"sensor-noise-modeling",children:"Sensor Noise Modeling"}),"\n",(0,a.jsx)(n.p,{children:"Realistic noise is crucial for sim-to-real transfer."}),"\n",(0,a.jsx)(n.h3,{id:"types-of-noise",children:"Types of Noise"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Sensor Noise] --\x3e B[Gaussian Noise]\r\n    A --\x3e C[Bias/Offset]\r\n    A --\x3e D[Drift]\r\n    A --\x3e E[Quantization]\r\n    \r\n    B --\x3e F[Random variations<br/>around true value]\r\n    C --\x3e G[Constant offset<br/>from true value]\r\n    D --\x3e H[Slowly changing<br/>offset over time]\r\n    E --\x3e I[Rounding to discrete<br/>values]\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"gaussian-noise-example",children:"Gaussian Noise Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- In Gazebo sensor definition --\x3e\r\n<noise>\r\n  <type>gaussian</type>\r\n  <mean>0.0</mean>  \x3c!-- No systematic error --\x3e\r\n  <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\r\n</noise>\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Effect"}),": Measurements vary randomly around true value."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"bias-and-drift",children:"Bias and Drift"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU accelerometer with bias --\x3e\r\n<linear_acceleration>\r\n  <x>\r\n    <noise type="gaussian">\r\n      <mean>0.0</mean>\r\n      <stddev>0.017</stddev>\r\n      <bias_mean>0.1</bias_mean>  \x3c!-- 0.1 m/s\xb2 constant bias --\x3e\r\n      <bias_stddev>0.001</bias_stddev>\r\n    </noise>\r\n  </x>\r\n</linear_acceleration>\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Effect"}),": Sensor consistently over/under-estimates by a fixed amount."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"custom-noise-in-unity",children:"Custom Noise in Unity"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:"public class NoisyDepthCamera : MonoBehaviour\r\n{\r\n    public float gaussianStdDev = 0.01f;  // 1cm\r\n    public float bias = 0.05f;  // 5cm constant offset\r\n    \r\n    float AddNoise(float trueDepth)\r\n    {\r\n        // Gaussian noise\r\n        float gaussian = GaussianRandom(0f, gaussianStdDev);\r\n        \r\n        // Add bias\r\n        float noisyDepth = trueDepth + gaussian + bias;\r\n        \r\n        // Clamp to valid range\r\n        return Mathf.Clamp(noisyDepth, 0.3f, 10f);\r\n    }\r\n    \r\n    float GaussianRandom(float mean, float stdDev)\r\n    {\r\n        // Box-Muller transform\r\n        float u1 = 1.0f - Random.value;\r\n        float u2 = 1.0f - Random.value;\r\n        float randStdNormal = Mathf.Sqrt(-2.0f * Mathf.Log(u1)) * Mathf.Sin(2.0f * Mathf.PI * u2);\r\n        return mean + stdDev * randStdNormal;\r\n    }\r\n}\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"multi-sensor-integration",children:"Multi-Sensor Integration"}),"\n",(0,a.jsxs)(n.p,{children:["Real robots use ",(0,a.jsx)(n.strong,{children:"sensor fusion"})," to combine data from multiple sensors."]}),"\n",(0,a.jsx)(n.h3,{id:"example-camera--lidar--imu",children:"Example: Camera + LiDAR + IMU"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class SensorFusion(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_fusion')\r\n        \r\n        # Subscribers\r\n        self.camera_sub = self.create_subscription(Image, '/camera/rgb/image_raw', self.camera_cb, 10)\r\n        self.lidar_sub = self.create_subscription(LaserScan, '/lidar/scan', self.lidar_cb, 10)\r\n        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_cb, 10)\r\n        \r\n        self.latest_image = None\r\n        self.latest_scan = None\r\n        self.latest_imu = None\r\n        \r\n        # Fused perception timer\r\n        self.timer = self.create_timer(0.1, self.fuse_sensors)  # 10 Hz\r\n    \r\n    def camera_cb(self, msg):\r\n        self.latest_image = msg\r\n    \r\n    def lidar_cb(self, msg):\r\n        self.latest_scan = msg\r\n    \r\n    def imu_cb(self, msg):\r\n        self.latest_imu = msg\r\n    \r\n    def fuse_sensors(self):\r\n        if all([self.latest_image, self.latest_scan, self.latest_imu]):\r\n            # Example fusion logic\r\n            # 1. Use IMU for orientation\r\n            # 2. Use LiDAR for obstacle detection\r\n            # 3. Use camera for object recognition\r\n            \r\n            self.get_logger().info('All sensors available - running fusion')\r\n            \r\n            # Your fusion algorithm here\r\n            pass\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"rviz2-visualization-configuration",children:"RViz2 Visualization Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"complete-rviz2-setup-for-sensor-debugging",children:"Complete RViz2 Setup for Sensor Debugging"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["Save this as ",(0,a.jsx)(n.code,{children:"sensors.rviz"}),":"]})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"Panels:\r\n  - Class: rviz_common/Displays\r\n    Name: Displays\r\n    \r\nVisualization Manager:\r\n  Global Options:\r\n    Fixed Frame: base_link\r\n    \r\n  Displays:\r\n    - Class: rviz_default_plugins/Image\r\n      Name: RGB Camera\r\n      Topic: /camera/rgb/image_raw\r\n      \r\n    - Class: rviz_default_plugins/Image\r\n      Name: Depth Camera\r\n      Topic: /depth_camera/depth/image_raw\r\n      \r\n    - Class: rviz_default_plugins/LaserScan\r\n      Name: 2D LiDAR\r\n      Topic: /lidar/scan\r\n      Size: 0.05\r\n      Color: 255; 0; 0\r\n      \r\n    - Class: rviz_default_plugins/PointCloud2\r\n      Name: 3D LiDAR\r\n      Topic: /lidar_3d/points\r\n      Size: 0.02\r\n      Color Transformer: AxisColor\r\n      Axis: Z\r\n      \r\n    - Class: rviz_default_plugins/Axes\r\n      Name: IMU Frame\r\n      Reference Frame: imu_link\r\n      Length: 0.3\r\n      Radius: 0.02\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Launch RViz with config:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"rviz2 -d sensors.rviz\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-update-rate-vs-real-time-factor",children:"Sensor Update Rate vs. Real-Time Factor"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph LR\r\n    A[Higher Update Rate] --\x3e B[More Data]\r\n    B --\x3e C[Better Perception]\r\n    B --\x3e D[Slower Simulation]\r\n    \r\n    E[Lower Update Rate] --\x3e F[Less Data]\r\n    F --\x3e G[Good Enough Perception]\r\n    F --\x3e H[Faster Simulation]\r\n    \r\n    style C fill:#90EE90\r\n    style D fill:#FFB6C6\r\n    style G fill:#FFFACD\r\n    style H fill:#90EE90\n"})}),"\n",(0,a.jsx)(n.h3,{id:"recommended-update-rates",children:"Recommended Update Rates"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Sensor"}),(0,a.jsx)(n.th,{children:"Update Rate"}),(0,a.jsx)(n.th,{children:"Rationale"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"RGB Camera"})}),(0,a.jsx)(n.td,{children:"30 Hz"}),(0,a.jsx)(n.td,{children:"Standard video frame rate"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Depth Camera"})}),(0,a.jsx)(n.td,{children:"15-30 Hz"}),(0,a.jsx)(n.td,{children:"Balance between data and computation"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"2D LiDAR"})}),(0,a.jsx)(n.td,{children:"10-40 Hz"}),(0,a.jsx)(n.td,{children:"Depends on robot speed"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"3D LiDAR"})}),(0,a.jsx)(n.td,{children:"10 Hz"}),(0,a.jsx)(n.td,{children:"Very compute-intensive"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"IMU"})}),(0,a.jsx)(n.td,{children:"100-200 Hz"}),(0,a.jsx)(n.td,{children:"High rate needed for accurate integration"})]})]})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"gazebo-performance-tips",children:"Gazebo Performance Tips"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Use GPU-accelerated sensors when possible --\x3e\r\n<sensor name="lidar" type="gpu_ray">  \x3c!-- NOT "ray" --\x3e\r\n  ...\r\n</sensor>\r\n\r\n\x3c!-- Reduce resolution for faster simulation --\x3e\r\n<horizontal>\r\n  <samples>180</samples>  \x3c!-- Instead of 360 --\x3e\r\n</horizontal>\r\n\r\n\x3c!-- Lower update rate during development --\x3e\r\n<update_rate>5.0</update_rate>  \x3c!-- Instead of 30 --\x3e\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"complete-sensor-suite-example",children:"Complete Sensor Suite Example"}),"\n",(0,a.jsx)(n.h3,{id:"humanoid-with-full-sensor-array",children:"Humanoid with Full Sensor Array"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Head camera (RGB + Depth) --\x3e\r\n<gazebo reference="head_camera_link">\r\n  <sensor name="rgbd_camera" type="depth">\r\n    <update_rate>30.0</update_rate>\r\n    \x3c!-- Configuration as shown earlier --\x3e\r\n  </sensor>\r\n</gazebo>\r\n\r\n\x3c!-- Chest LiDAR (for navigation) --\x3e\r\n<gazebo reference="chest_lidar_link">\r\n  <sensor name="chest_lidar" type="ray">\r\n    <update_rate>10.0</update_rate>\r\n    \x3c!-- 2D LiDAR config --\x3e\r\n  </sensor>\r\n</gazebo>\r\n\r\n\x3c!-- Torso IMU (for balance) --\x3e\r\n<gazebo reference="torso_imu_link">\r\n  <sensor name="torso_imu" type="imu">\r\n    <update_rate>100.0</update_rate>\r\n    \x3c!-- IMU config --\x3e\r\n  </sensor>\r\n</gazebo>\r\n\r\n\x3c!-- Foot force-torque sensors --\x3e\r\n<gazebo reference="left_foot_link">\r\n  <sensor name="left_foot_ft" type="force_torque">\r\n    <update_rate>100.0</update_rate>\r\n    <plugin name="ft_controller" filename="libgazebo_ros_ft_sensor.so">\r\n      <ros>\r\n        <namespace>left_foot</namespace>\r\n        <remapping>~/out:=wrench</remapping>\r\n      </ros>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.h3,{id:"-key-takeaways",children:"\ud83c\udfaf Key Takeaways"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cameras"})," provide rich visual data for object detection and tracking"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LiDAR"})," excels at distance measurement and mapping (2D/3D)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IMU"})," is essential for orientation and acceleration estimation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor noise"})," modeling is critical for realistic sim-to-real transfer"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"RViz2"})," is the go-to tool for visualizing sensor data in ROS 2"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update rates"})," must balance perception quality with simulation speed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-sensor fusion"})," combines complementary sensor strengths"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"-sensor-comparison",children:"\ud83d\udcca Sensor Comparison"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Choose Sensor] --\x3e B{Application?}\r\n    B --\x3e|Object Recognition| C[RGB Camera]\r\n    B --\x3e|3D Reconstruction| D[Depth Camera]\r\n    B --\x3e|Obstacle Avoidance| E[2D LiDAR]\r\n    B --\x3e|Mapping| F[3D LiDAR]\r\n    B --\x3e|Balance/Localization| G[IMU]\r\n    B --\x3e|Contact Detection| H[Force-Torque]\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,a.jsx)(n.h3,{id:"task-create-multi-sensor-humanoid",children:"Task: Create Multi-Sensor Humanoid"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Add RGB camera to robot head (640x480, 30Hz)"}),"\n",(0,a.jsx)(n.li,{children:"Add 2D LiDAR to chest (360 samples, 10Hz)"}),"\n",(0,a.jsx)(n.li,{children:"Add IMU to torso (100Hz with realistic noise)"}),"\n",(0,a.jsx)(n.li,{children:"Launch in Gazebo and verify all topics publish"}),"\n",(0,a.jsx)(n.li,{children:"Visualize all sensors in RViz2 simultaneously"}),"\n",(0,a.jsxs)(n.li,{children:["Record 30 seconds of sensor data using ",(0,a.jsx)(n.code,{children:"ros2 bag"})]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["All sensor topics appear in ",(0,a.jsx)(n.code,{children:"ros2 topic list"})]}),"\n",(0,a.jsx)(n.li,{children:"Camera shows valid RGB image in rqt_image_view"}),"\n",(0,a.jsx)(n.li,{children:"LiDAR shows obstacles in RViz2"}),"\n",(0,a.jsx)(n.li,{children:"IMU publishes orientation quaternion"}),"\n",(0,a.jsx)(n.li,{children:"Simulation maintains RTF > 0.8"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Estimated Time"}),": 40 minutes"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"further-resources",children:"Further Resources"}),"\n",(0,a.jsx)(n.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\ud83d\udcd8 ",(0,a.jsx)(n.a,{href:"https://gazebosim.org/api/sensors/6/tutorials.html",children:"Gazebo Sensors"})," - All sensor types"]}),"\n",(0,a.jsxs)(n.li,{children:["\ud83d\udcf7 ",(0,a.jsx)(n.a,{href:"https://github.com/ros-perception/image_pipeline",children:"ROS 2 Image Pipeline"})," - Camera processing"]}),"\n",(0,a.jsxs)(n.li,{children:["\ud83d\udce1 ",(0,a.jsx)(n.a,{href:"https://github.com/ros2/common_interfaces/tree/rolling/sensor_msgs",children:"sensor_msgs"})," - ROS 2 message formats"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"tutorials",children:"Tutorials"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\ud83c\udfa5 ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/watch?v=%3Cexample%3E",children:"RViz2 Sensor Visualization"})," - Complete guide"]}),"\n",(0,a.jsxs)(n.li,{children:["\ud83d\udcdd ",(0,a.jsx)(n.a,{href:"https://github.com/Unity-Technologies/com.unity.perception",children:"Unity Perception Tutorial"})," - Synthetic data generation"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"tools",children:"Tools"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\ud83d\udee0\ufe0f ",(0,a.jsx)(n.a,{href:"https://github.com/facontidavide/PlotJuggler",children:"PlotJuggler"})," - Visualize sensor data over time"]}),"\n",(0,a.jsxs)(n.li,{children:["\ud83d\udee0\ufe0f ",(0,a.jsx)(n.a,{href:"https://foxglove.dev/",children:"Foxglove Studio"})," - Modern ROS 2 visualization"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,a.jsxs)(n.p,{children:["In ",(0,a.jsx)(n.strong,{children:"Chapter 10"}),", you'll learn ",(0,a.jsx)(n.strong,{children:"Sim-to-Real Transfer Techniques"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understanding reality gap sources in depth"}),"\n",(0,a.jsx)(n.li,{children:"Implementing domain randomization for robust policies"}),"\n",(0,a.jsx)(n.li,{children:"System identification for accurate parameter estimation"}),"\n",(0,a.jsx)(n.li,{children:"Progressive transfer learning strategies"}),"\n",(0,a.jsx)(n.li,{children:"Validating simulation results on real hardware"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Complete the simulation loop and prepare for real-world deployment! \ud83d\ude80\ud83e\udd16"}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.admonition,{title:"Chapter Completion",type:"note",children:(0,a.jsxs)(n.p,{children:["\u2705 You've completed Chapter 9: Sensor Simulation",(0,a.jsx)(n.br,{}),"\n","\u23f1\ufe0f Estimated time to complete: 75 minutes",(0,a.jsx)(n.br,{}),"\n","\ud83d\udcca Progress: Module 2 - Chapter 4 of 5"]})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(6540);const a={},s=i.createContext(a);function t(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);