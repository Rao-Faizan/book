"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8186],{2874:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module4/chapter16","title":"Chapter 16: Voice-to-Action with OpenAI Whisper","description":"Implement speech-to-text for robots using OpenAI Whisper, create wake words, and map voice commands to ROS 2 actions","source":"@site/docs/module4/chapter16.mdx","sourceDirName":"module4","slug":"/module4/chapter16","permalink":"/book/docs/module4/chapter16","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/book/tree/master/docs/module4/chapter16.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Chapter 16: Voice-to-Action with OpenAI Whisper","description":"Implement speech-to-text for robots using OpenAI Whisper, create wake words, and map voice commands to ROS 2 actions","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: VLA & Generative AI","permalink":"/book/docs/category/module-4-vla--generative-ai"},"next":{"title":"Chapter 17: LLMs for Cognitive Planning","permalink":"/book/docs/module4/chapter17"}}');var t=r(4848),s=r(8453);const o={title:"Chapter 16: Voice-to-Action with OpenAI Whisper",description:"Implement speech-to-text for robots using OpenAI Whisper, create wake words, and map voice commands to ROS 2 actions",sidebar_position:1},l="Chapter 16: Voice-to-Action with OpenAI Whisper",a={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"The Voice Interface",id:"the-voice-interface",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Step 1: Setting Up Audio Input",id:"step-1-setting-up-audio-input",level:2},{value:"Installation",id:"installation",level:3},{value:"Audio Capture Node",id:"audio-capture-node",level:3},{value:"Step 2: Wake Word Detection",id:"step-2-wake-word-detection",level:2},{value:"Installation",id:"installation-1",level:3},{value:"Wake Word Node",id:"wake-word-node",level:3},{value:"Step 3: Integrating OpenAI Whisper",id:"step-3-integrating-openai-whisper",level:2},{value:"Option A: Whisper API (Cloud)",id:"option-a-whisper-api-cloud",level:3},{value:"Option B: Local Whisper (GPU)",id:"option-b-local-whisper-gpu",level:3},{value:"Step 4: Connecting the Pipeline",id:"step-4-connecting-the-pipeline",level:2},{value:"Intent Parser (Simple Regex)",id:"intent-parser-simple-regex",level:3},{value:"The Full Node",id:"the-full-node",level:3},{value:"Step 5: Adding a Voice (TTS)",id:"step-5-adding-a-voice-tts",level:2},{value:"Installation",id:"installation-2",level:3},{value:"TTS Wrapper",id:"tts-wrapper",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Task: The &quot;Obedient Robot&quot;",id:"task-the-obedient-robot",level:3},{value:"Summary",id:"summary",level:2},{value:"\ud83c\udfaf Key Takeaways",id:"-key-takeaways",level:3}];function c(e){const n={admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-16-voice-to-action-with-openai-whisper",children:"Chapter 16: Voice-to-Action with OpenAI Whisper"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand"})," the architecture of OpenAI Whisper and its advantages for robotics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Capture"})," live audio streams in ROS 2 using PyAudio and ",(0,t.jsx)(n.code,{children:"audio_common"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," a Wake Word engine (Porcupine) to activate your robot"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transcribe"})," speech to text using both Whisper API and local GPU inference"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Extract"}),' intents from text (e.g., "Move Forward" from "Please go ahead")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control"})," your robot's movement via voice commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthesize"})," speech (TTS) for robot feedback"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Microphone connected (or simulator audio input), Module 1 (ROS 2 Basics)",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"Estimated Time"}),": 90 minutes"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"the-voice-interface",children:"The Voice Interface"}),"\n",(0,t.jsxs)(n.p,{children:["Voice is the most natural interface for human-robot interaction (HRI). Historically, speech recognition was fragile and required cloud connection. ",(0,t.jsx)(n.strong,{children:"OpenAI Whisper"})," changed this by offering human-level accuracy even in noisy environments, with an open-source model that runs locally on GPUs (like Jetson)."]}),"\n",(0,t.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph LR\r\n    A[Microphone] --\x3e B[Wake Word<br/>(Porcupine)]\r\n    B --\x3e|Triggered| C[Audio Buffer]\r\n    C --\x3e|Audio Clip| D[Whisper Model]\r\n    D --\x3e|Text| E[Intent Parser]\r\n    E --\x3e|Command| F[ROS 2 Action]\r\n    F --\x3e G[Robot Motor]\r\n    E --\x3e H[TTS Engine]\r\n    H --\x3e|Audio| I[Speaker]\r\n    \r\n    style D fill:#76B900\r\n    style E fill:#F4A261\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"step-1-setting-up-audio-input",children:"Step 1: Setting Up Audio Input"}),"\n",(0,t.jsxs)(n.p,{children:["We need a flexible way to capture audio in Python and publish it or process it. ",(0,t.jsx)(n.code,{children:"PyAudio"})," is the standard library."]}),"\n",(0,t.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install system dependencies (PortAudio)\r\nsudo apt-get install python3-pyaudio portaudio19-dev\r\n\r\n# Install Python libraries\r\npip install pyaudio numpy\n"})}),"\n",(0,t.jsx)(n.h3,{id:"audio-capture-node",children:"Audio Capture Node"}),"\n",(0,t.jsxs)(n.p,{children:["Let's create a ROS 2 node ",(0,t.jsx)(n.code,{children:"audio_capture.py"})," that listens to the microphone."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nimport pyaudio\r\nimport numpy as np\r\nfrom std_msgs.msg import Int16MultiArray\r\n\r\nclass AudioCaptureNode(Node):\r\n    def __init__(self):\r\n        super().__init__('audio_capture')\r\n        self.pub = self.create_publisher(Int16MultiArray, 'audio_raw', 10)\r\n        \r\n        # Audio Params\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000  # Whisper prefers 16kHz\r\n        \r\n        self.p = pyaudio.PyAudio()\r\n        self.stream = self.p.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        self.timer = self.create_timer(0.01, self.capture_callback)\r\n        self.get_logger().info(\"Microphone initialized\")\r\n\r\n    def capture_callback(self):\r\n        try:\r\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\r\n            # Convert to numpy and publish\r\n            audio_data = np.frombuffer(data, dtype=np.int16)\r\n            msg = Int16MultiArray()\r\n            msg.data = audio_data.tolist()\r\n            self.pub.publish(msg)\r\n        except Exception as e:\r\n            self.get_logger().error(f\"Error capturing audio: {e}\")\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = AudioCaptureNode()\r\n    rclpy.spin(node)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"step-2-wake-word-detection",children:"Step 2: Wake Word Detection"}),"\n",(0,t.jsxs)(n.p,{children:["Streaming ",(0,t.jsx)(n.em,{children:"all"}),' audio to a large model is inefficient and privacy-invasive. Robots needs a "Wake Word" (like "Hey Robot") to start listening. We\'ll use ',(0,t.jsx)(n.strong,{children:"Porcupine"})," by Picovoice (lightweight, runs on CPU)."]}),"\n",(0,t.jsx)(n.h3,{id:"installation-1",children:"Installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install pvporcupine pyaudio\n"})}),"\n",(0,t.jsx)(n.h3,{id:"wake-word-node",children:"Wake Word Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nimport pvporcupine\r\nimport pyaudio\r\nimport struct\r\nfrom std_msgs.msg import Bool\r\n\r\nclass WakeWordNode(Node):\r\n    def __init__(self):\r\n        super().__init__('wake_word_detector')\r\n        self.pub = self.create_publisher(Bool, '/voice/wake_word', 10)\r\n        \r\n        # Initialize Porcupine (default keyword: 'porcupine', 'jarvis', 'picovoice')\r\n        self.porcupine = pvporcupine.create(keywords=[\"jarvis\"])\r\n        \r\n        self.pa = pyaudio.PyAudio()\r\n        self.audio_stream = self.pa.open(\r\n            rate=self.porcupine.sample_rate,\r\n            channels=1,\r\n            format=pyaudio.paInt16,\r\n            input=True,\r\n            frames_per_buffer=self.porcupine.frame_length\r\n        )\r\n        \r\n        self.listening_timer = self.create_timer(0.05, self.listen_loop)\r\n        self.get_logger().info(\"Listening for 'Jarvis'...\")\r\n\r\n    def listen_loop(self):\r\n        pcm = self.audio_stream.read(self.porcupine.frame_length)\r\n        pcm = struct.unpack_from(\"h\" * self.porcupine.frame_length, pcm)\r\n        \r\n        keyword_index = self.porcupine.process(pcm)\r\n        if keyword_index >= 0:\r\n            self.get_logger().info(\"Wake Word Detected!\")\r\n            msg = Bool()\r\n            msg.data = True\r\n            self.pub.publish(msg)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"step-3-integrating-openai-whisper",children:"Step 3: Integrating OpenAI Whisper"}),"\n",(0,t.jsx)(n.p,{children:"When the wake word is detected, we record audio for ~5 seconds (or until silence) and send it to Whisper."}),"\n",(0,t.jsx)(n.h3,{id:"option-a-whisper-api-cloud",children:"Option A: Whisper API (Cloud)"}),"\n",(0,t.jsx)(n.p,{children:"Higher accuracy (Large-v3 model), slower latency, costs money."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\r\nclient = OpenAI(api_key="sk-...")\r\n\r\ndef transcribe_cloud(audio_file_path):\r\n    with open(audio_file_path, "rb") as audio_file:\r\n        transcript = client.audio.transcriptions.create(\r\n            model="whisper-1", \r\n            file=audio_file\r\n        )\r\n    return transcript.text\n'})}),"\n",(0,t.jsx)(n.h3,{id:"option-b-local-whisper-gpu",children:"Option B: Local Whisper (GPU)"}),"\n",(0,t.jsxs)(n.p,{children:["Free, private, works offline. Use ",(0,t.jsx)(n.code,{children:"distil-whisper"})," or ",(0,t.jsx)(n.code,{children:"tiny/base"})," models for speed on Jetson."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper torch\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import whisper\r\nmodel = whisper.load_model(\"base\")  # 'base' or 'small' is good for robotics\r\n\r\ndef transcribe_local(audio_file_path):\r\n    result = model.transcribe(audio_file_path)\r\n    return result[\"text\"]\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"step-4-connecting-the-pipeline",children:"Step 4: Connecting the Pipeline"}),"\n",(0,t.jsxs)(n.p,{children:["Let's create ",(0,t.jsx)(n.code,{children:"voice_commander.py"}),". This node:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Subscribes to ",(0,t.jsx)(n.code,{children:"/voice/wake_word"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Records audio for 4 seconds using PyAudio."}),"\n",(0,t.jsx)(n.li,{children:"Transcribes using Whisper."}),"\n",(0,t.jsx)(n.li,{children:"Parses text intent."}),"\n",(0,t.jsxs)(n.li,{children:["Publishes to ",(0,t.jsx)(n.code,{children:"/cmd_vel"})," to move the robot."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"intent-parser-simple-regex",children:"Intent Parser (Simple Regex)"}),"\n",(0,t.jsxs)(n.p,{children:["Robots need ",(0,t.jsx)(n.strong,{children:"Actions"}),", not just text."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def parse_command(text):\r\n    text = text.lower()\r\n    msg = Twist()\r\n    \r\n    if "forward" in text or "go" in text:\r\n        msg.linear.x = 0.5\r\n        return msg, "Moving forward"\r\n        \r\n    elif "back" in text:\r\n        msg.linear.x = -0.3\r\n        return msg, "Backing up"\r\n        \r\n    elif "left" in text:\r\n        msg.angular.z = 1.0\r\n        return msg, "Turning left"\r\n        \r\n    elif "right" in text:\r\n        msg.angular.z = -1.0\r\n        return msg, "Turning right"\r\n        \r\n    elif "stop" in text or "halt" in text:\r\n        return msg, "Stopping"  # Zero velocity\r\n        \r\n    return None, "I didn\'t understand that."\n'})}),"\n",(0,t.jsx)(n.h3,{id:"the-full-node",children:"The Full Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import Bool, String\r\nfrom geometry_msgs.msg import Twist\r\nimport pyaudio\r\nimport wave\r\nimport whisper\r\nimport os\r\n\r\nclass VoiceCommander(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_commander\')\r\n        \r\n        # ROS 2 Interfaces\r\n        self.sub_wake = self.create_subscription(Bool, \'/voice/wake_word\', self.wake_cb, 10)\r\n        self.pub_vel = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.pub_feedback = self.create_publisher(String, \'/voice/feedback\', 10)\r\n        \r\n        # Whisper Model\r\n        self.get_logger().info("Loading Whisper model...")\r\n        self.model = whisper.load_model("base")\r\n        self.get_logger().info("Whisper model loaded!")\r\n        \r\n        self.recording = False\r\n\r\n    def wake_cb(self, msg):\r\n        if msg.data and not self.recording:\r\n            self.recording = True\r\n            self.record_and_process()\r\n\r\n    def record_and_process(self):\r\n        self.get_logger().info("Recording command...")\r\n        \r\n        # Record 3 seconds\r\n        CHUNK = 1024\r\n        FORMAT = pyaudio.paInt16\r\n        CHANNELS = 1\r\n        RATE = 16000\r\n        RECORD_SECONDS = 3\r\n        WAVE_OUTPUT_FILENAME = "/tmp/command.wav"\r\n\r\n        p = pyaudio.PyAudio()\r\n        stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\r\n        frames = []\r\n\r\n        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\r\n            data = stream.read(CHUNK)\r\n            frames.append(data)\r\n\r\n        stream.stop_stream()\r\n        stream.close()\r\n        p.terminate()\r\n\r\n        # Save WAV\r\n        wf = wave.open(WAVE_OUTPUT_FILENAME, \'wb\')\r\n        wf.setnchannels(CHANNELS)\r\n        wf.setsampwidth(p.get_sample_size(FORMAT))\r\n        wf.setframerate(RATE)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n\r\n        # Transcribe\r\n        self.get_logger().info("Transcribing...")\r\n        result = self.model.transcribe(WAVE_OUTPUT_FILENAME)\r\n        text = result["text"]\r\n        self.get_logger().info(f"Heard: \'{text}\'")\r\n\r\n        # Execute\r\n        cmd_vel, response = self.parse_command(text)\r\n        \r\n        # Publish Feedback\r\n        self.pub_feedback.publish(String(data=response))\r\n        \r\n        if cmd_vel:\r\n            self.pub_vel.publish(cmd_vel)\r\n            self.get_logger().info(f"Executed: {response}")\r\n            \r\n        self.recording = False\r\n\r\n    def parse_command(self, text):\r\n        # (Insert parsing logic from above)\r\n        text = text.lower()\r\n        msg = Twist()\r\n        \r\n        if "forward" in text or "go" in text:\r\n            msg.linear.x = 0.5\r\n            return msg, "Moving forward"\r\n        elif "stop" in text:\r\n            return msg, "Stopping"\r\n        return None, "Command parsing failed"\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"step-5-adding-a-voice-tts",children:"Step 5: Adding a Voice (TTS)"}),"\n",(0,t.jsxs)(n.p,{children:["Communication is two-way. When the robot understands, it should reply. We'll use ",(0,t.jsx)(n.strong,{children:"gTTS"})," (Google TTS, online) or ",(0,t.jsx)(n.strong,{children:"pyttsx3"})," (Offline)."]}),"\n",(0,t.jsx)(n.h3,{id:"installation-2",children:"Installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install gTTS playsound\n"})}),"\n",(0,t.jsx)(n.h3,{id:"tts-wrapper",children:"TTS Wrapper"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from gtts import gTTS\r\nimport os\r\n\r\ndef speak(text):\r\n    tts = gTTS(text=text, lang=\'en\')\r\n    tts.save("/tmp/response.mp3")\r\n    os.system("mpg321 /tmp/response.mp3")  # Linux player\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Integrate into Node"}),":\r\nInside ",(0,t.jsx)(n.code,{children:"voice_commander"}),", subscribe to ",(0,t.jsx)(n.code,{children:"/voice/feedback"})," and call ",(0,t.jsx)(n.code,{children:"speak(msg.data)"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,t.jsx)(n.h3,{id:"task-the-obedient-robot",children:'Task: The "Obedient Robot"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Create a complete voice-control loop for a simulated robot in Gazebo (or Isaac Sim)."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Launch"})," your robot simulation (from Module 2/3)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run"})," the ",(0,t.jsx)(n.code,{children:"WakeWordNode"}),' ("Jarvis").']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run"})," the ",(0,t.jsx)(n.code,{children:"VoiceCommander"})," node."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test Commands"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Jarvis... Move forward" -> Robot should move.'}),"\n",(0,t.jsx)(n.li,{children:'"Jarvis... Stop" -> Robot updates.'}),"\n",(0,t.jsx)(n.li,{children:'"Jarvis... Turn around" -> Robot rotates.'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"System wakes up reliably."}),"\n",(0,t.jsx)(n.li,{children:'Transcribes "Forward", "Back", "Left", "Right", "Stop" correctly.'}),"\n",(0,t.jsx)(n.li,{children:"Robot executes movement in simulation."}),"\n",(0,t.jsx)(n.li,{children:'Robot speaks back "Moving forward" etc.'}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.h3,{id:"-key-takeaways",children:"\ud83c\udfaf Key Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Wake Words"})," (Porcupine) save compute/battery by listening only when needed."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper"})," provides robust, open-source speech-to-text suitable for robotics."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PyAudio"})," bridges the gap between hardware functionality and Python logic."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Parsing"})," is the bridge between natural language (Text) and Robot Action (Twist)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TTS"})," completes the HRI loop, making the robot feel interactive."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next Chapter"}),": Simple keyword matching is limited. In Chapter 17, we will replace our basic ",(0,t.jsx)(n.code,{children:"if-else"})," parser with a ",(0,t.jsx)(n.strong,{children:"Large Language Model (LLM)"}),' to understand complex, unstructured commands like "Check the kitchen for obstacles."']}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.admonition,{title:"Chapter Completion",type:"note",children:(0,t.jsxs)(n.p,{children:["\u2705 You've completed Chapter 16: Voice-to-Action with OpenAI Whisper",(0,t.jsx)(n.br,{}),"\n","\u23f1\ufe0f Estimated time to complete: 90 minutes",(0,t.jsx)(n.br,{}),"\n","\ud83d\udcca Progress: Module 4 - Chapter 1 of 5"]})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var i=r(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);