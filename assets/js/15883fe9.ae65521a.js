"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4380],{5561:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4/chapter17","title":"Chapter 17: LLMs for Cognitive Planning","description":"Empower robots with human-like reasoning using Large Language Models (LLMs) to plan tasks, handle ambiguity, and generate actionable ROS 2 commands","source":"@site/docs/module4/chapter17.mdx","sourceDirName":"module4","slug":"/module4/chapter17","permalink":"/book/docs/module4/chapter17","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/book/tree/master/docs/module4/chapter17.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Chapter 17: LLMs for Cognitive Planning","description":"Empower robots with human-like reasoning using Large Language Models (LLMs) to plan tasks, handle ambiguity, and generate actionable ROS 2 commands","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 16: Voice-to-Action with OpenAI Whisper","permalink":"/book/docs/module4/chapter16"},"next":{"title":"Chapter 18: Multi-Modal Robot Interaction","permalink":"/book/docs/module4/chapter18"}}');var s=r(4848),i=r(8453);const o={title:"Chapter 17: LLMs for Cognitive Planning",description:"Empower robots with human-like reasoning using Large Language Models (LLMs) to plan tasks, handle ambiguity, and generate actionable ROS 2 commands",sidebar_position:2},l="Chapter 17: LLMs for Cognitive Planning",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"From Commands to Reasoning",id:"from-commands-to-reasoning",level:2},{value:"The Cognitive Architecture",id:"the-cognitive-architecture",level:2},{value:"Step 1: Defining the Robot Ontology",id:"step-1-defining-the-robot-ontology",level:2},{value:"Step 2: OpenAI API Integration",id:"step-2-openai-api-integration",level:2},{value:"Installation",id:"installation",level:3},{value:"The Planner Class",id:"the-planner-class",level:3},{value:"Step 3: The Action Executor Node",id:"step-3-the-action-executor-node",level:2},{value:"Step 4: Prompt Engineering for Robotics",id:"step-4-prompt-engineering-for-robotics",level:2},{value:"1. Chain of Thought (CoT)",id:"1-chain-of-thought-cot",level:3},{value:"2. State Awareness",id:"2-state-awareness",level:3},{value:"3. Error Recovery",id:"3-error-recovery",level:3},{value:"Step 5: Complete Cognitive Node",id:"step-5-complete-cognitive-node",level:2},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Task: The &quot;Smart&quot; Butler",id:"task-the-smart-butler",level:3},{value:"Cost &amp; Latency Considerations",id:"cost--latency-considerations",level:2},{value:"Summary",id:"summary",level:2},{value:"\ud83c\udfaf Key Takeaways",id:"-key-takeaways",level:3}];function d(e){const n={admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-17-llms-for-cognitive-planning",children:"Chapter 17: LLMs for Cognitive Planning"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Understand"})," how LLMs (GPT-4, LLaMA) can serve as high-level planners for robotics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design"})," effective prompts to translate natural language into robot actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Define"}),' a "Robot Action Ontology" that LLMs can understand']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integrate"})," OpenAI API with ROS 2 for real-time decision making"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," structured output parsing (JSON/Function Calling) ensures reliable execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build"}),' a "Cognitive Node" that converts "Clean the room" into a sequence of atomic tasks']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Handle"})," errors and re-planning when the robot fails"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites"}),": Chapter 16 (Voice Interface), OpenAI API Key (or local LLM)",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"from-commands-to-reasoning",children:"From Commands to Reasoning"}),"\n",(0,s.jsxs)(n.p,{children:['In Chapter 16, we mapped "Move Forward" directly to ',(0,s.jsx)(n.code,{children:"cmd_vel"}),'. But what about "Fetch me a snack"?\r\nA simple ',(0,s.jsx)(n.code,{children:"if-else"})," parser fails here. The robot needs ",(0,s.jsx)(n.strong,{children:"Reasoning"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'What is a "snack"? (Apple, Chips)'}),"\n",(0,s.jsx)(n.li,{children:"Where are they usually kept? (Kitchen, Table)"}),"\n",(0,s.jsx)(n.li,{children:"How do I get there? (Navigate)"}),"\n",(0,s.jsx)(n.li,{children:"How do I pick it up? (Grasp)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LLMs"})," provide this common-sense reasoning capability."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"the-cognitive-architecture",children:"The Cognitive Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["We don't train the LLM to ",(0,s.jsx)(n.em,{children:"control motors"}),". We use it as a ",(0,s.jsx)(n.strong,{children:"High-Level Planner"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[User Command<br/>'Clean the table'] --\x3e B[LLM Planner<br/>(Brain)]\r\n    B --\x3e|Context + Prompt| C[Reasoning Chain]\r\n    C --\x3e|Action Sequence| D[Action Dispatcher]\r\n    \r\n    D --\x3e E[Nav2<br/>(Legs)]\r\n    D --\x3e F[MoveIt<br/>(Arms)]\r\n    D --\x3e G[Perception<br/>(Eyes)]\r\n    \r\n    E --\x3e H[Success/Fail]\r\n    \r\n    H --\x3e|Feedback| B\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-1-defining-the-robot-ontology",children:"Step 1: Defining the Robot Ontology"}),"\n",(0,s.jsxs)(n.p,{children:["The LLM doesn't know what your robot ",(0,s.jsx)(n.em,{children:"can"})," do. We must define an ",(0,s.jsx)(n.strong,{children:"API"})," for it."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"You are a home assistant robot.\r\nYou can execute the following atomic actions:\r\n\r\n1. navigate(location: str) -> bool\r\n   - Moves to a named location.\r\n   - Locations: [kitchen, living_room, bedroom]\r\n\r\n2. pick(object_name: str) -> bool\r\n   - Grasps an object.\r\n   - Objects must be visible.\r\n\r\n3. place(location_name: str) -> bool\r\n   - Places held object.\r\n\r\n4. look_for(object_name: str) -> bool\r\n   - Rotates to find an object.\r\n\r\nOutput your plan as a Python list of function calls.\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-2-openai-api-integration",children:"Step 2: OpenAI API Integration"}),"\n",(0,s.jsxs)(n.p,{children:["Let's build a Python class ",(0,s.jsx)(n.code,{children:"LLMPlanner"})," that wraps the OpenAI API."]}),"\n",(0,s.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai pydantic\n"})}),"\n",(0,s.jsx)(n.h3,{id:"the-planner-class",children:"The Planner Class"}),"\n",(0,s.jsxs)(n.p,{children:["We use ",(0,s.jsx)(n.strong,{children:"JSON Mode"})," or ",(0,s.jsx)(n.strong,{children:"Function Calling"})," to ensure the LLM outputs valid code, not chatty text."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\r\nimport json\r\nfrom openai import OpenAI\r\n\r\nclass LLMPlanner:\r\n    def __init__(self, api_key):\r\n        self.client = OpenAI(api_key=api_key)\r\n        self.system_prompt = """\r\n        You are a robot planner. Convert user instructions into a sequence of actions.\r\n        Available actions:\r\n        - navigate(location)\r\n        - look_for(object)\r\n        - pick(object)\r\n        - place(location)\r\n        \r\n        Return a JSON object with a key "plan" containing a list of strings.\r\n        Example: {"plan": ["navigate(\'kitchen\')", "pick(\'apple\')"]}\r\n        """\r\n\r\n    def generate_plan(self, user_command):\r\n        response = self.client.chat.completions.create(\r\n            model="gpt-4-turbo",\r\n            messages=[\r\n                {"role": "system", "content": self.system_prompt},\r\n                {"role": "user", "content": user_command}\r\n            ],\r\n            response_format={"type": "json_object"}\r\n        )\r\n        \r\n        content = response.choices[0].message.content\r\n        return json.loads(content)["plan"]\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-3-the-action-executor-node",children:"Step 3: The Action Executor Node"}),"\n",(0,s.jsxs)(n.p,{children:["Now we need a ROS 2 node that receives this plan and executes it, one step at a time. It acts as the ",(0,s.jsx)(n.strong,{children:"Dispatcher"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass ActionExecutor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'action_executor\')\r\n        self.sub_plan = self.create_subscription(String, \'/llm/plan\', self.plan_cb, 10)\r\n        \r\n        # Mock Clients for actual robot capabilities\r\n        self.get_logger().info("Action Executor Ready")\r\n\r\n    def plan_cb(self, msg):\r\n        import json\r\n        plan = json.loads(msg.data)\r\n        self.get_logger().info(f"Received Plan: {plan}")\r\n        self.execute_sequence(plan)\r\n\r\n    def execute_sequence(self, plan):\r\n        for action in plan:\r\n            self.get_logger().info(f"Executing: {action}")\r\n            success = self.parse_and_run(action)\r\n            if not success:\r\n                self.get_logger().error(f"Action failed: {action}")\r\n                return # Stop execution\r\n        self.get_logger().info("Plan Completed Successfully!")\r\n\r\n    def parse_and_run(self, action_str):\r\n        # Primitive parsing (in production, use Regex or AST)\r\n        import time\r\n        time.sleep(1) # Simulate execution time\r\n        return True # Mock success\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-4-prompt-engineering-for-robotics",children:"Step 4: Prompt Engineering for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"To make the LLM reliable, we use specific prompting techniques."}),"\n",(0,s.jsx)(n.h3,{id:"1-chain-of-thought-cot",children:"1. Chain of Thought (CoT)"}),"\n",(0,s.jsx)(n.p,{children:'Ask the LLM to "think" before acting.'}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt"}),": \"First, analyze the user's intent. Then, check if it's feasible. Finally, list the actions.\""]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-state-awareness",children:"2. State Awareness"}),"\n",(0,s.jsx)(n.p,{children:"Inject the robot's current state into the system prompt."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Prompt Update"}),':\r\n"Current State: [Location: Kitchen, Holding: None, Battery: 80%]"']}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-error-recovery",children:"3. Error Recovery"}),"\n",(0,s.jsx)(n.p,{children:"If an action fails, feed the error back to the LLM."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"User"}),': "Put the apple on the table."\r\n',(0,s.jsx)(n.strong,{children:"LLM"}),": ",(0,s.jsx)(n.code,{children:"navigate('table')"})," -> ",(0,s.jsx)(n.code,{children:"pick('apple')"}),"\r\n",(0,s.jsx)(n.strong,{children:"Robot"}),": Error: ",(0,s.jsx)(n.code,{children:"pick('apple')"})," failed. Object not found.\r\n",(0,s.jsx)(n.strong,{children:"LLM (Re-plan)"}),': "I see. I will look for it first." -> ',(0,s.jsx)(n.code,{children:"look_for('apple')"})," -> ",(0,s.jsx)(n.code,{children:"pick('apple')"})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-5-complete-cognitive-node",children:"Step 5: Complete Cognitive Node"}),"\n",(0,s.jsxs)(n.p,{children:["Let's combine Chapter 16 (Voice) + this Chapter into a ",(0,s.jsx)(n.code,{children:"cognitive_agent.py"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom llm_planner import LLMPlanner\r\n\r\nclass CognitiveAgent(Node):\r\n    def __init__(self):\r\n        super().__init__('cognitive_agent')\r\n        self.planner = LLMPlanner(api_key=\"sk-...\")\r\n        \r\n        # Input: Transcribed Text (from Ch 16)\r\n        self.sub_text = self.create_subscription(String, '/voice/text', self.text_cb, 10)\r\n        \r\n        # Output: JSON Plan\r\n        self.pub_plan = self.create_publisher(String, '/llm/plan', 10)\r\n\r\n    def text_cb(self, msg):\r\n        command = msg.data\r\n        self.get_logger().info(f\"Thinking about: {command}\")\r\n        \r\n        try:\r\n            plan = self.planner.generate_plan(command)\r\n            # Publish as JSON string\r\n            import json\r\n            plan_msg = String()\r\n            plan_msg.data = json.dumps(plan)\r\n            self.pub_plan.publish(plan_msg)\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f\"Planning failed: {e}\")\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = CognitiveAgent()\r\n    rclpy.spin(node)\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,s.jsx)(n.h3,{id:"task-the-smart-butler",children:'Task: The "Smart" Butler'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Scenario"}),": Simulation (Gazebo/Isaac Sim) with a robot, a kitchen, and a living room.\r\n",(0,s.jsx)(n.strong,{children:"Command"}),': "I\'m thirsty."']}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Setup"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Run ",(0,s.jsx)(n.code,{children:"ActionExecutor"})," (mock or real Nav2 clients)."]}),"\n",(0,s.jsxs)(n.li,{children:["Run ",(0,s.jsx)(n.code,{children:"CognitiveAgent"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:['Publish "I\'m thirsty" to ',(0,s.jsx)(n.code,{children:"/voice/text"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observation"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LLM should infer: Thirsty -> Need Drink -> Water/Juice."}),"\n",(0,s.jsxs)(n.li,{children:["Plan: ",(0,s.jsx)(n.code,{children:"navigate('kitchen')"})," -> ",(0,s.jsx)(n.code,{children:"pick('water')"})," -> ",(0,s.jsx)(n.code,{children:"navigate('living_room')"})," -> ",(0,s.jsx)(n.code,{children:"place('table')"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Executor runs these steps."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LLM generates a valid logic chain (~3-4 steps)."}),"\n",(0,s.jsx)(n.li,{children:"Robot moves between locations in simulation."}),"\n",(0,s.jsx)(n.li,{children:'System handles "impossible" requests (e.g., "Fly to the moon") by politely declining via TTS.'}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"cost--latency-considerations",children:"Cost & Latency Considerations"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Intelligent?"}),(0,s.jsx)(n.th,{children:"Latency"}),(0,s.jsx)(n.th,{children:"Cost"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"GPT-4"})}),(0,s.jsx)(n.td,{children:"Very High"}),(0,s.jsx)(n.td,{children:"3-5s"}),(0,s.jsx)(n.td,{children:"$$$"}),(0,s.jsx)(n.td,{children:"Complex planning, recovery"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"GPT-3.5"})}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"1-2s"}),(0,s.jsx)(n.td,{children:"$"}),(0,s.jsx)(n.td,{children:"Standard commands"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama-3-8b"})}),(0,s.jsx)(n.td,{children:"Med"}),(0,s.jsx)(n.td,{children:"<1s (Local)"}),(0,s.jsx)(n.td,{children:"Free"}),(0,s.jsx)(n.td,{children:"Simple tasks, privacy"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Best Practice"}),": Use a ",(0,s.jsx)(n.strong,{children:"Hybrid Approach"}),'. Use a small, fast model for simple commands ("Stop", "Go forward") and escalate to GPT-4 only for complex reasoning.']}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.h3,{id:"-key-takeaways",children:"\ud83c\udfaf Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLMs"}),' act as the "Pre-frontal Cortex" of the robot, handling reasoning and planning.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Ontologies"})," act as the API layer between vague language and strict code."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering"})," (Context, CoT) is critical for reliability."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback Loops"}),' allows the LLM to fix its own mistakes ("Grounding").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Structured Output"})," (JSON) prevents parsing errors."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Chapter"}),": We've mastered Voice and Reasoning. Now we add ",(0,s.jsx)(n.strong,{children:"Vision"}),". A robot that can ",(0,s.jsx)(n.em,{children:"see"}),' "the red cup" and talk about it requires ',(0,s.jsx)(n.strong,{children:"Vision-Language Models (VLMs)"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.admonition,{title:"Chapter Completion",type:"note",children:(0,s.jsxs)(n.p,{children:["\u2705 You've completed Chapter 17: LLMs for Cognitive Planning",(0,s.jsx)(n.br,{}),"\n","\u23f1\ufe0f Estimated time to complete: 120 minutes",(0,s.jsx)(n.br,{}),"\n","\ud83d\udcca Progress: Module 4 - Chapter 2 of 5"]})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var t=r(6540);const s={},i=t.createContext(s);function o(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);